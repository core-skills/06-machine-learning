{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Performance of the Classifier - Imbalanced datasets\n",
    "\n",
    "In this notebook we are going to explore the [Bosch Production Line Performance](https://www.kaggle.com/c/bosch-production-line-performance/data) dataset, which was part of the [Kaggle](https://www.kaggle.com) competition sponsored by Bosch in 2016. The data for this competition represents measurements of components as they move through Bosch's production lines. Each component has a unique Id. The goal is to predict which components will fail quality control (represented by a **Response** = 1).\n",
    "\n",
    "The dataset contains an extremely large number of anonymized features. Features are named according to a convention that tells you the production line, the station on the line, and a feature number. E.g. L3_S36_F3939 is a feature measured on line 3, station 36, and is feature number 3939.\n",
    "\n",
    "As it is a very large dataset, we are going to use a reduced training dataset. In 2016 the original dataset was one of the largest datasets (in terms of number of features) ever hosted on Kaggle. Besides that, this dataset is highly **imbalanced**. Given our time and computational resources restrictions, we selected the most promissing features (according to these [kernels](https://www.kaggle.com/c/bosch-production-line-performance/kernels)) and cleaned the dataset (mainly replacing the NaN values by the mean by using [this](http://scikit-learn.org/stable/modules/impute.html#impute) approach). We also removed the temporal dependencies in order to facilitate the illustratation of the concepts we want to discuss.\n",
    "\n",
    "The original dataset is composed by 1,183,747 samples, distributed as:\n",
    " - 1,176,868 samples for Response 0\n",
    " - 6,879 samples for Response 1\n",
    " \n",
    "We random sampled this dataset to a much smaller one, making the effort of matching the original distribution. The reduced dataset has the following distribution:\n",
    " - 19,700 samples for Response 0\n",
    " - 300 samples for Response 1\n",
    " \n",
    "Note that the reduced dataset still **highy imbalanced**. In this notebook, we are going to show the effect of the evaluation performance metrics discussed in the previous notebook under this highly imbalanced dataset. We will implement ideas about how we can deal with such scenario in order to improve the generalisation of the classifier. For instance, it is clear that a simple accuracy won't give us reliable results (we could just pick the most frequent class and have similar accuracies).\n",
    "\n",
    "In order to have a clean notebook, some functions are implemented in the file *utils.py* (e.g., plot_decision_boundary). We are not going to discuss the implementation aspects of these functions as it is not the scope, but you can to explore and read the content of the functions later on.\n",
    "\n",
    "Summary:\n",
    " - [Data Pre-processing](#data_preprocessing)\n",
    " - [Building the Random Forest Classifier](#models)\n",
    " - [Quantifying the Quality of Predictions](#pred)\n",
    "     - [Accuracy](#accuracy)\n",
    "     - [Confusion Matrix](#confusion)\n",
    "     - [Precision and Recall](#precision)\n",
    "     - [F1-score](#f1score)\n",
    "     - [Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC)](#roc)\n",
    "     - [K-fold Cross-Validation](#kfold)\n",
    "     - [Discussion](#mat)\n",
    " - [Dealing with Imbalanced Dataset](#imbalanced)\n",
    "  \n",
    "__All the libraries used in this notebook are <font color='red'>Open Source</font>__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "<a id=data_preprocessing></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np  # written in C, is faster and robust library for numerical and matrix operations\n",
    "import pandas as pd # data manipulation library, it is widely used for data analysis and relies on numpy library.\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # plot nicely =)\n",
    "\n",
    "from sklearn.model_selection import train_test_split #split arrays or matrices into random train and test subsets\n",
    "from sklearn.preprocessing import StandardScaler #Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "# Auxiliar functions\n",
    "from utils import *\n",
    "\n",
    "# the following to lines will tell to the python kernel to always update the kernel for every utils.py\n",
    "# modification, without the need of restarting the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# using the 'inline' backend, your matplotlib graphs will be included in your notebook, next to the code\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the reduced version of the dataset. It is all prepared for you in \"reduced_train.csv\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading dataset\n",
    "data = pd.read_csv(\"../data/reduced_train.csv\", index_col=0, header=0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analising the distribution of the classes (Response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Response # set the variable 'y' to store the labels\n",
    "list = ['Response']\n",
    "data = data.drop(list,axis = 1 ) # removing the column Response from the dataframe\n",
    "data.head()\n",
    "\n",
    "# counting the number of unique labels in the dataset.\n",
    "target_count = y.value_counts()\n",
    "print('Response 0:', target_count[0])\n",
    "print('Response 1:', target_count[1])\n",
    "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n",
    "\n",
    "ax = sns.countplot(x=y, label=\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this dataset is highly imbalanced!!! Generally, we will have much more data from one class (Reponse 0) and much less data from the other class (Response 1). There are some techniques to deal with such imbalanced dataset, we are going to discuss them further in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data train 70% and test 30%. You can try other splits here.\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# normalising the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train) \n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "y_train = y_train.values # converting to numpy array\n",
    "y_test = y_test.values # converting to numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Random Forest Classifier\n",
    "<a id=models></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # implements random decision forest.\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=10) #10 trees in the forest \n",
    "clr_rf = clf_rf.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying the Quality of Predictions\n",
    "<a id=pred></a>\n",
    "\n",
    "The performance assessment of the **classifiers** is extremely important in practice, as this provide insights of how the classifier performs with new data, in which me measure the **generalisation error**.\n",
    "\n",
    "Summary:\n",
    " - [Accuracy](#accuracy)\n",
    " - [Confusion Matrix](#confusion)\n",
    " - [Precision and Recall](#precision)\n",
    " - [F1-score](#f1score)\n",
    " - [Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC)](#roc)\n",
    " - [K-fold Cross-Validation](#kfold)\n",
    " - [Learning Curve](#learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "<a id=accuracy></a>\n",
    "\n",
    "Computes the accuracy of the classifier by  using the following equation:\n",
    "\n",
    "$$accuracy = \\frac{1}{N}\\sum_{i=0}^N{1*(\\hat{y}==y)}$$\n",
    "where $y$ is the true label, $\\hat{y}$ the predicted label and $N$ is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# we are using prediction computed previsously\n",
    "prediction = clr_rf.predict(x_test_scaled)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, prediction)\n",
    "\n",
    "print('Accuracy: ', accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just get 98.6% of **accuracy**. Let's use other tools to quantify the quality of predictions for our classifier and check if this accuracy is reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "<a id=confusion></a>\n",
    "\n",
    "The confusion matrix is a tool/technique for summarising the performance of the classifier. We can have better insights about when the classifier is getting right and what are the types of errors it is making. It can be very useful for a further improvement of our models.\n",
    "\n",
    "<img src=\"imgs/cm.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "In the image above, we have: \n",
    " - **True Positive (TP):** correctly predicted event values.\n",
    " - **False Positive (FP):** incorrectly predicted event values.\n",
    " - **True Negative (TN):** correctly predicted no-event values.\n",
    " - **False Negative (FN):** incorrectly predicted no-event values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  confusion_matrix: computes confusion matrix to evaluate the accuracy of a classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, prediction)\n",
    "sns.heatmap(cm, annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is strange with this confustion matrix?\n",
    "\n",
    "Tip: there is 19,700 samples for response 0 (True Positives) and 300 samples for response 1 (True Negatives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall\n",
    "<a id=precision></a>\n",
    "\n",
    "The confusion matrix give us a lot of information, but sometimes we may need a better metric in order to evaluate the classifier accuracy. Precision and recall scores are two metrics naturally provided by confusion matrix evaluation.\n",
    "\n",
    "\n",
    "Precision is given by the equation:\n",
    "\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "where $TP$ is the number of True Positives and $FP$ is the number of False Positives.\n",
    "\n",
    "\n",
    "Recall is given by the equation:\n",
    "\n",
    "$$recall  = \\frac{TP}{TP + FN}$$\n",
    "where $TP$ is the number of True Positives and $FN$ is the number of False Negatives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "#   precision_score: computes precision score\n",
    "#   recall_score: computes recall score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# we are using prediction computed previsously\n",
    "precision_rf = precision_score(y_test, prediction)\n",
    "recall_rf = recall_score(y_test, prediction)\n",
    "\n",
    "print('Precision: ', precision_rf)\n",
    "print('Recall: ', recall_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What? Why precision and recall scores are so different, and also different from accuracy score? Why recall score is so small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "<a id=f1score></a>\n",
    "\n",
    "Also known as **F-Measure**, can be interpreted as a weighted average of the precision and recall. The formula for F1-score is:\n",
    "\n",
    "$$f_1 = 2 * \\frac{precision * recall}{precision+recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score # computes the f1 score\n",
    "\n",
    "# we are using prediction computed previsously\n",
    "f1_score_rf = f1_score(y_test, prediction)\n",
    "print('F1-score: ', f1_score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again confirms that the classifier is leading to a lot of misclassifications. In our case, our classifier can't be applied for quality control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC)\n",
    "<a id=roc></a>\n",
    "\n",
    "#### Receiver Operating Characteristic (ROC) \n",
    "ROC curve plot the *true positive rate-TPR* (a.k.a. *recall score*) against the *false negative rate-FPR*. FPR is the ratio of negative instances that are incorrectly classifier as positive.\n",
    "\n",
    "\n",
    "#### Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "#  roc_curve: computes the receiver operating characteristic curve\n",
    "#  roc_auc_score: computes Area Under the Receiver Operating Characteristic Curve score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# computing and plotting the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, prediction)\n",
    "\n",
    "plot_roc_curve(fpr=fpr, tpr=tpr)\n",
    "\n",
    "# computing the AUC (Area Under the Curve)\n",
    "auc_rf = roc_auc_score(y_test, prediction)\n",
    "print('AUC: ', auc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Could you interpret what means AUC equal 0.5? In this problem we have just two classes (Response 0 and Reponse 1), the probability to have a Response 0 is given by $P(Response = 0) = 0.5$ and to have a Response 1 is  $P(Response = 1) = 0.5$. This means that, if we do a random guess the response we can have 50% of accuracy. Having said that, is it worth to build a model with 50% of accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross-Validation\n",
    "<a id=kfold></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the library StratifiedKFold for this task.\n",
    "#   The StratifiedKFold performs stratified sampling to produce folds that contain a representative ratio of each \n",
    "#   class. At each iteraction the code create a clone of the classifier, train that clone on the training \n",
    "#   folds, and makes prediction on the test fold. \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "for train, test in cv.split(x_train_scaled, y_train):\n",
    "    clone_clf = clone(clf_rf)\n",
    "    \n",
    "    # splitting the training set\n",
    "    x_train_folds = x_train_scaled[train]\n",
    "    y_train_folds = y_train[train]\n",
    "    x_test_folds = x_train_scaled[test]\n",
    "    y_test_folds = y_train[test]\n",
    "    \n",
    "    # building the classifier\n",
    "    clone_clf.fit(x_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(x_test_folds)\n",
    "    \n",
    "    # computing the auc score\n",
    "    auc_scores.append(roc_auc_score(y_test_folds, y_pred))\n",
    "    \n",
    "    # computing the f1-score\n",
    "    f1_scores.append(f1_score(y_test_folds, y_pred))\n",
    "    \n",
    "\n",
    "fold = 1\n",
    "print('Fold\\tAUC\\tF1-score')\n",
    "for auc, f1 in zip(auc_scores, f1_scores):\n",
    "    print('{}\\t{:.3f}\\t{:.3f}'.format(fold, auc, f1))\n",
    "    fold += 1\n",
    "\n",
    "print()\n",
    "print('Mean AUC: {:.3f}'.format(np.array(auc_scores).mean()))\n",
    "print('Mean F1-score: {:.3f}'.format(np.array(f1_scores).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even by using K-fold cross-validation, we were not able to build a better classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Discussion\n",
    "<a id=mat></a>\n",
    "\n",
    "In scenarios where we aim at detecting samples of a minority class (Response = 1 in our case), we are usually concerned to achieve a better recall than a better precision. You can imagine situations where it is usually more expensive to miss a negative sample (false positive) than to falsely classify a positive sample. Therefore, when comparing approaches to deal with imbalanced datasets, it is crucial to consider metrics beyond accuracy (as we saw above) such as recall and AUC. Other metrics, such as the Matthews Correlation Coefficient (MCC)](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) can be compute as well, as this last one can deal with imbalanced dataset and will give more importance to negative samples, which is our case. \n",
    "\n",
    "The MCC equation is given by:\n",
    "\n",
    "$$MCC = \\frac{TP*TN - FP*FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n",
    "\n",
    "The [Matthews Correlation Coefficient (MCC)](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) is, in essence, a correlation coefficient value between -1 and +1:\n",
    " - MCC equals +1 represents a perfect prediction\n",
    " - MCC equals 0 an average random prediction\n",
    " - MCC equals -1 an inverse prediction. \n",
    " \n",
    "In scikit learn, you can compute this coefficient as follows:\n",
    "```python\n",
    "from sklearn.metrics import matthews_corrcoef #Implements the Matthews correlation coefficient\n",
    "\n",
    "mcc_rf = matthews_corrcoef(y_test, prediction)\n",
    "print('Matthews correlation coefficient is: ', mcc_rf)\n",
    "```\n",
    "\n",
    "Try the code above if you which."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Imbalanced Dataset\n",
    "<a id=imbalanced></a>\n",
    "\n",
    "### Cost-sensitive Learning\n",
    "\n",
    "Generally, the misclassification are treated equally, which causes problems in imbalaced scenarios, as there is not a reward for identifying the minority class over the majority class. In *cost-sensitive learning* approach, in order to increase the true positive rate, the misclassification for the minority class are heavly penalised more than the missclassifications of the majority class. For very large dataset, this approach is costly computationally.\n",
    "\n",
    "### Sampling\n",
    "\n",
    "Sampling is a simple way to fix imbalanced datasets, the basic idea is to create or remove samples in order to balance them. Two main approaches can be used in order to balance the dataset:\n",
    "\n",
    " - Oversampling, which creates samples for the minority class. In practice, this techniques can lead to overfitting, since we are introducing duplicated samples.\n",
    " - Undersampling, which removes samples from the majority class. In practice, this can enp up by removing important samples that are more discriminative between the two classes.\n",
    "\n",
    "The Figure below illustrates the basic idea behind sampling (click [here](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets) to see the reference).\n",
    "<img src=\"imgs/resampling.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Several powerful samples methods have been proposed in order to avoid the practical problems of oversampling or undersampling. In this notebook we are going to focus in an specific oversampling method known as [Synthetic Minority Over-sampling Technique (SMOTE)](https://jair.org/index.php/jair/article/view/10302). This method creates new samples of the minotiry class by forming convex combination of the neighboring samples.\n",
    "\n",
    "<img src=\"imgs/smote.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "As can be seen in the figure, SMOTE effectively draws lines between minority smaples in the feature space, and samples along these lines. The key of this approach is that it creates new synthetic samples rather than duplicates existing samples, this allows datasets to be balanced without including too much overfitting. However, this does not prevent all overfitting, as these are still created from existing data points.\n",
    "\n",
    "In order to apply the SMOTE approach we are going to use the Python library [imbalanced-learn](https://imbalanced-learn.org/stable/). It is compatible with scikit-learn and is part of scikit-learn-contrib projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE # implements SMOTE\n",
    "\n",
    "sampling = SMOTE()\n",
    "\n",
    "Xs, ys = sampling.fit_resample(data, y) # create the new synthetic samples # LLPS2022-02-03: fit_sample -> fit_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the new class distributions\n",
    "unique, counts = np.unique(ys, return_counts=True)\n",
    "\n",
    "print('Response == {}:'.format(unique[0]), counts[0])\n",
    "print('Response == {}:'.format(unique[1]), counts[1])\n",
    "print('Proportion:', round(counts[0] / counts[1], 2), ': 1')\n",
    "\n",
    "ax = sns.countplot(x=ys, label=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data train 70% and test 30%. You can try other splits here.\n",
    "x_train_new, x_test_new, y_train_new, y_test_new = train_test_split(Xs, ys, test_size=0.3, random_state=42)\n",
    "\n",
    "# normalising the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_new)\n",
    "\n",
    "x_train_new = scaler.transform(x_train_new)\n",
    "x_test_new = scaler.transform(x_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the classifier\n",
    "from sklearn.ensemble import RandomForestClassifier # implements random decision forest.\n",
    "\n",
    "clf_rf_i = RandomForestClassifier(n_estimators=10) #10 trees in the forest \n",
    "clf_rf_i.fit(x_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf_rf_i.predict(x_test_new)\n",
    "\n",
    "# computing and plotting the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_new, prediction)\n",
    "\n",
    "plot_roc_curve(fpr=fpr, tpr=tpr)\n",
    "\n",
    "# computing the AUC (Area Under the Curve)\n",
    "auc_rf_i = roc_auc_score(y_test_new, prediction)\n",
    "print('AUC: ', auc_rf_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "plot_learning_curve(clf_rf_i, title='Random Decision Forest Learning Curve', \n",
    "                    X=x_train_new, \n",
    "                    y=y_train_new, \n",
    "                    ylim=(0.85, 1.01), \n",
    "                    cv=cv, \n",
    "                    n_jobs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases it can let to overfitting. However, more powerful techniques were proposed in order to reduce it. For example, the technique [Adaptive Synthetic (ADASYN)](https://sci2s.ugr.es/keel/pdf/algorithm/congreso/2008-He-ieee.pdf), which is an improved version of SMOTE. This approach, after creating the new samples adds a random small values to the samples thus making them more realistic. In this sense, instead of all the samples being linearly correlated to the neighbors, they have a little more variance in them.\n",
    "\n",
    "This technique is implemented in the library **imblearn**, and can be used as follows:\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import ADASYN # implements SMOTE\n",
    "\n",
    "sampling = ADASYN()\n",
    "\n",
    "Xs, ys = sampling.fit_sample(data, y) # create the new synthetic samples\n",
    "```\n",
    "\n",
    "Try the code above and see if you can find some difference in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
