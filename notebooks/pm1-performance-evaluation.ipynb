{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Performance of the Classifier\n",
    "\n",
    "In this notebook we are going to evaluate the performance of the K-NN classifier built for the **Iron Ore** dataset. In order to have a clean notebook, some functions are implemented in the file *utils.py* (e.g., plot_learning_curve). We are not going to discuss the implementation aspects of these functions as it is not the scope, but you can explore and read the content of the functions later on.\n",
    "\n",
    "Summary:\n",
    " - [Data Pre-processing](#data_preprocessing)\n",
    " - [Building the K-Nearest Neighbors Classifier](#models)\n",
    " - [Quantifying the Quality of Predictions](#pred)\n",
    "     - [Accuracy](#accuracy)\n",
    "     - [Confusion Matrix](#confusion)\n",
    "     - [Precision and Recall](#precision)\n",
    "     - [F1-score](#f1score)\n",
    "     - [Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC)](#roc)\n",
    "     - [K-fold Cross-Validation](#kfold)\n",
    "     - [Learning Curve](#learning)\n",
    "  \n",
    "__All the libraries used in this notebook are <font color='red'>Open Source</font>__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "<a id=data_preprocessing></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np  # written in C, is faster and robust library for numerical and matrix operations\n",
    "import pandas as pd # data manipulation library, it is widely used for data analysis and relies on numpy library.\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # plot nicely =)\n",
    "\n",
    "from sklearn.model_selection import train_test_split #split arrays or matrices into random train and test subsets\n",
    "from sklearn.preprocessing import StandardScaler #Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "# Auxiliar functions\n",
    "from utils import *\n",
    "\n",
    "# the following to lines will tell to the python kernel to always update the kernel for every utils.py\n",
    "# modification, without the need of restarting the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# using the 'inline' backend, your matplotlib graphs will be included in your notebook, next to the code\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first example, we will work on the same dataset as the previous notebook, the **Iron Ore** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading dataset\n",
    "df = pd.read_csv('../data/iron_ore_study.csv')\n",
    "\n",
    "# Splits from oscar Fe>60%, SiO2<9, Al2O3<2, P<0.08\n",
    "split_points = [\n",
    "    ('FE', 60, [False, True]),\n",
    "    ('SIO2', 9, [True, False]),\n",
    "    ('AL2O3', 2, [True, False]),\n",
    "    ('P', 0.08, [True, False]),  \n",
    "]\n",
    "\n",
    "# It's ore if everything is True\n",
    "df['is_ore'] = np.vstack([\n",
    "    pd.cut(df[elem], bins=[0, split, 100], labels=is_ore)\n",
    "    for elem, split, is_ore in split_points\n",
    "]).sum(axis=0) == 4\n",
    "\n",
    "y = df.is_ore # set the variable 'y' to store the labels\n",
    "# removing is_ore from the dataframe \n",
    "list = ['is_ore']\n",
    "df = df.drop(list,axis = 1 )\n",
    "\n",
    "# split data train 70% and test 30%. You can try other splits here.\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, y, test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "y_train = y_train.values # converting to numpy array\n",
    "y_test = y_test.values # converting to numpy array\n",
    "\n",
    "# normalising the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train) # not considering the label is_ore\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the K-Nearest Neighbors Classifier\n",
    "<a id=models></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier #Classifier implementing the k-nearest neighbors vote.\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=3)   \n",
    "\n",
    "clf_knn.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying the Quality of Predictions\n",
    "<a id=pred></a>\n",
    "\n",
    "The performance assessment of the **classifiers** is extremely important in practice, as this provide insights of how the classifier performs with new data, in which me measure the **generalisation error**.\n",
    "\n",
    "Summary:\n",
    " - [Accuracy](#accuracy)\n",
    " - [Confusion Matrix](#confusion)\n",
    " - [Precision and Recall](#precision)\n",
    " - [F1-score](#f1score)\n",
    " - [Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC)](#roc)\n",
    " - [K-fold Cross-Validation](#kfold)\n",
    " - [Learning Curve](#learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "<a id=accuracy></a>\n",
    "\n",
    "Computes the accuracy of the classifier by  using the following equation:\n",
    "\n",
    "$$accuracy = \\frac{1}{N}\\sum_{i=0}^N{1*(\\hat{y}==y)}$$\n",
    "where $y$ is the true label, $\\hat{y}$ the predicted label and $N$ is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# we are using prediction computed previsously\n",
    "prediction = clf_knn.predict(x_test_scaled)\n",
    "\n",
    "accuracy_knn = accuracy_score(y_test, prediction)\n",
    "\n",
    "print('Accuracy: ', accuracy_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We just got 97.4% of **accuracy**. Can we assume that we are done? Let's use other tools to quantify the quality of predictions for our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "<a id=confusion></a>\n",
    "\n",
    "The confusion matrix is a tool/technique for summarising the performance of the classifier. We can have better insights about when the classifier is getting right and what are the types of errors it is making. It can be very useful for a further improvement of our models.\n",
    "\n",
    "<img src=\"imgs/cm.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "In the image above, we have: \n",
    " - **True Positive (TP):** correctly predicted event values.\n",
    " - **False Positive (FP):** incorrectly predicted event values.\n",
    " - **True Negative (TN):** correctly predicted no-event values.\n",
    " - **False Negative (FN):** incorrectly predicted no-event values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  confusion_matrix: computes confusion matrix to evaluate the accuracy of a classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, prediction)\n",
    "sns.heatmap(cm, annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall\n",
    "<a id=precision></a>\n",
    "\n",
    "The confusion matrix give us a lot of information, but sometimes we may need a better metric in order to evaluate the classifier accuracy. Precision and recall scores are two metrics naturally provided by confusion matrix evaluation.\n",
    "\n",
    "\n",
    "Precision is given by the equation:\n",
    "\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "where $TP$ is the number of True Positives and $FP$ is the number of False Positives.\n",
    "\n",
    "\n",
    "Recall is given by the equation:\n",
    "\n",
    "$$recall  = \\frac{TP}{TP + FN}$$\n",
    "where $TP$ is the number of True Positives and $FN$ is the number of False Negatives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "#   precision_score: computes precision score\n",
    "#   recall_score: computes recall score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# we are using prediction computed previsously\n",
    "precision_knn = precision_score(y_test, prediction)\n",
    "recall_knn = recall_score(y_test, prediction)\n",
    "\n",
    "print('Precision: ', precision_knn)\n",
    "print('Recall: ', recall_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "<a id=f1score></a>\n",
    "\n",
    "Also known as **F-Measure**, can be interpreted as a weighted average of the precision and recall. The formula for F1-score is:\n",
    "\n",
    "$$f_1 = 2 * \\frac{precision * recall}{precision+recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score # computes the f1 score\n",
    "\n",
    "# we are using prediction computed previsously\n",
    "f1_score_knn = f1_score(y_test, prediction)\n",
    "print('F1-score: ', f1_score_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC)\n",
    "<a id=roc></a>\n",
    "\n",
    "#### Receiver Operating Characteristic (ROC) \n",
    "ROC curve plot the *true positive rate-TPR* (a.k.a. *recall score*) against the *false negative rate-FPR*. FPR is the ratio of negative instances that are incorrectly classifier as positive.\n",
    "\n",
    "\n",
    "#### Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "#  roc_curve: computes the receiver operating characteristic curve\n",
    "#  roc_auc_score: computes Area Under the Receiver Operating Characteristic Curve score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# computing and plotting the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, prediction)\n",
    "\n",
    "plot_roc_curve(fpr=fpr, tpr=tpr)\n",
    "\n",
    "# computing the AUC (Area Under the Curve)\n",
    "auc_knn = roc_auc_score(y_test, prediction)\n",
    "print('AUC: ', auc_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-fold Cross-Validation\n",
    "<a id=kfold></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the library StratifiedKFold for this task.\n",
    "#   The StratifiedKFold performs stratified sampling to produce folds that contain a representative ratio of each \n",
    "#   class. At each iteraction the code create a clone of the classifier, train that clone on the training \n",
    "#   folds, and makes prediction on the test fold. \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "for train, test in cv.split(x_train_scaled, y_train):\n",
    "    clone_clf = clone(clf_knn)\n",
    "    \n",
    "    # splitting the training set\n",
    "    x_train_folds = x_train_scaled[train]\n",
    "    y_train_folds = y_train[train]\n",
    "    x_test_folds = x_train_scaled[test]\n",
    "    y_test_folds = y_train[test]\n",
    "    \n",
    "    # building the classifier\n",
    "    clone_clf.fit(x_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(x_test_folds)\n",
    "    \n",
    "    # computing the auc score\n",
    "    auc_scores.append(roc_auc_score(y_test_folds, y_pred))\n",
    "    \n",
    "    # computing the f1-score\n",
    "    f1_scores.append(f1_score(y_test_folds, y_pred))\n",
    "    \n",
    "\n",
    "fold = 1\n",
    "print('Fold\\tAUC\\tF1-score')\n",
    "for auc, f1 in zip(auc_scores, f1_scores):\n",
    "    print('{}\\t{:.3f}\\t{:.3f}'.format(fold, auc, f1))\n",
    "    fold += 1\n",
    "\n",
    "print()\n",
    "print('Mean AUC: {:.3f}'.format(np.array(auc_scores).mean()))\n",
    "print('Mean F1-score: {:.3f}'.format(np.array(f1_scores).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning Curve\n",
    "<a id=learning></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "plot_learning_curve(clf_knn, title='K-NN Learning Curve', \n",
    "                    X=x_train_scaled, \n",
    "                    y=y_train, \n",
    "                    ylim=(0.85, 1.01), \n",
    "                    cv=cv, \n",
    "                    n_jobs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Turn:\n",
    "\n",
    "1) Build a classifier, you can choise a Random Forest or SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) Build the classifier\n",
    "    \n",
    "   For example, for Random Forest you can do:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier # implements random decision forest.\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=20)  # 20 trees in the forest.    \n",
    "clf_rf.fit(x_train_scaled, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) Evaluate the classifier\n",
    "\n",
    "In the case of Random Forest, you can do:\n",
    "```python\n",
    "pred_rf = clf_rf.predict(x_test_scaled)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Evaluate the performance of the classifier by using the metrics presented in this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1) Precision and Recall\n",
    "\n",
    "For example:\n",
    "```python\n",
    "precision_rf = precision_score(y_test, pred_rf)\n",
    "recall_rf = recall_score(y_test, pred_rf)\n",
    "\n",
    "print('Precision: ', precision_rf)\n",
    "print('Recall: ', recall_rf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2) F1-score\n",
    "\n",
    "For example:\n",
    "```python\n",
    "f1_score_rf = f1_score(y_test, pred_rf)\n",
    "print('F1-score: ', f1_score_rf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3) Plot the ROC curve and compute the AUC score\n",
    "\n",
    "For example:\n",
    "```python\n",
    "# computing and plotting the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_rf)\n",
    "plot_roc_curve(fpr=fpr, tpr=tpr)\n",
    "\n",
    "# computing the AUC (Area Under the Curve)\n",
    "auc_rf = roc_auc_score(y_test, pred_rf)\n",
    "print('AUC: ', auc_rf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Can we do better? \n",
    "\n",
    "To answer this question, compute the learning curve:\n",
    "\n",
    "For example:\n",
    "```python\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "\n",
    "plot_learning_curve(clf_rf, title='Random Forest Learning Curve', \n",
    "                    X=x_train_scaled, \n",
    "                    y=y_train, \n",
    "                    ylim=(0.85, 1.01), \n",
    "                    cv=cv, \n",
    "                    n_jobs=5)\n",
    "```\n",
    "\n",
    "**PS: if you are using SVM, choose n_splits small**.\n",
    "\n",
    "\n",
    "If you whish,  compute the K-fold Stratified Cross validation to get better insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Compare the evaluation results for the different classifier with the previous K-NN classifier. Could you find some insights about the performance of both classfiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
